name: "ðŸ”¦ Lighthouse Performance Monitoring"

on:
  # Run every 2 hours on develop
  schedule:
    - cron: "0 */2 * * *" # Every 2 hours for develop
    # TODO: Enable main branch monitoring when production has dev auth bypass
    # - cron: "0 6 * * *"    # Daily at 6 AM for main

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      target_url:
        description: "Target URL to test"
        required: false
        default: "https://staging.violetvault.app"
        type: string

# Prevent duplicate runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  lighthouse-audit:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main' # Run on both develop and main branches

    steps:
      - name: "ðŸ—ï¸ Checkout code"
        uses: actions/checkout@v6

      - name: "ðŸ”§ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"

      - name: "ðŸ“¦ Install dependencies"
        run: npm ci --legacy-peer-deps

      - name: "ðŸ§ª Run test suite"
        continue-on-error: true
        run: |
          echo "ðŸ§ª Running Vitest test suite..."

          # Initialize test results tracking
          TEST_FAILURES=""
          TEST_SUCCESS_COUNT=0
          TEST_TOTAL_COUNT=0
          TEST_PASSED=true

          # Run tests and capture results (continue on error to ensure lighthouse runs)
          set +e  # Don't exit on error
          npm run test -- --reporter=verbose --reporter=json --outputFile=test-results.json 2>&1 | tee test-output.log
          TEST_EXIT_CODE=$?
          set -e  # Re-enable exit on error

          if [ $TEST_EXIT_CODE -eq 0 ]; then
            echo "âœ… All tests passed"
            TEST_SUCCESS_COUNT=$(grep -o "passed" test-output.log | wc -l || echo "0")
            TEST_TOTAL_COUNT=$TEST_SUCCESS_COUNT
          else
            echo "âŒ Some tests failed (exit code: $TEST_EXIT_CODE)"
            TEST_PASSED=false

            # Extract test statistics from output
            TEST_TOTAL_COUNT=$(grep -E "Tests:|Test Files:" test-output.log | head -1 | grep -o '[0-9]\+' | head -1 || echo "0")
            TEST_SUCCESS_COUNT=$(grep -o "passed" test-output.log | wc -l || echo "0")

            # Capture failed test information with more context
            echo "ðŸ” Extracting detailed test failure information..."
            TEST_FAILURES=$(grep -A 10 -B 5 "FAIL\|âœ•\|failed\|Error:" test-output.log | head -100 || echo "Test failures detected but details not captured")

            # Also capture specific error patterns
            echo "ðŸ“‹ Failed test summary:" > test-failures-detailed.log
            echo "========================" >> test-failures-detailed.log
            grep -E "(FAIL|âœ•|failed|Error:|expect.*received)" test-output.log >> test-failures-detailed.log 2>/dev/null || echo "No specific patterns found" >> test-failures-detailed.log
            echo "" >> test-failures-detailed.log
            echo "Full test output (last 200 lines):" >> test-failures-detailed.log
            echo "====================================" >> test-failures-detailed.log
            tail -200 test-output.log >> test-failures-detailed.log
          fi

          # Store results for later steps
          echo "TEST_PASSED=$TEST_PASSED" >> $GITHUB_ENV
          echo "TEST_SUCCESS_COUNT=$TEST_SUCCESS_COUNT" >> $GITHUB_ENV
          echo "TEST_TOTAL_COUNT=$TEST_TOTAL_COUNT" >> $GITHUB_ENV
          echo "TEST_EXIT_CODE=$TEST_EXIT_CODE" >> $GITHUB_ENV
          echo "TEST_FAILURES<<EOF" >> $GITHUB_ENV
          echo "$TEST_FAILURES" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

          echo "ðŸ”„ Test suite complete - continuing with lighthouse tests regardless of test results"

      - name: "ðŸŽ« Generate dev auth token"
        id: auth
        run: |
          TOKEN=$(DEV_AUTH_SECRET="${{ secrets.DEV_AUTH_SECRET }}" node scripts/generate-dev-auth-token.js | grep -A1 "ðŸ”— Token:" | tail -1)
          echo "token=$TOKEN" >> $GITHUB_OUTPUT
        env:
          DEV_AUTH_SECRET: ${{ secrets.DEV_AUTH_SECRET }}

      - name: "ðŸ” Authenticate with dev auth bypass"
        run: |
          # For now, only targeting dev environment
          TARGET_URL="${{ github.event.inputs.target_url || 'https://staging.violetvault.app' }}"
          echo "ðŸ” Authenticating with $TARGET_URL..."

          # Authenticate and get cookies
          curl -c cookies.txt -b cookies.txt -L "$TARGET_URL/__dev_auth?token=${{ steps.auth.outputs.token }}&target=/dashboard"

          echo "âœ… Authentication complete"

      - name: "ðŸ“‚ Create lighthouse directory and copy test reports"
        run: |
          mkdir -p lighthouse-results

          # Copy test reports to lighthouse results for unified reporting
          echo "ðŸ“‹ Copying test reports to lighthouse results..."
          cp test-output.log lighthouse-results/ 2>/dev/null || echo "No test-output.log found"
          cp test-results.json lighthouse-results/ 2>/dev/null || echo "No test-results.json found"
          cp test-failures-detailed.log lighthouse-results/ 2>/dev/null || echo "No test-failures-detailed.log found"

          # Create a test summary file
          cat > lighthouse-results/test-summary.md << EOF
          # ðŸ§ª Test Execution Summary

          **Status:** $([ "$TEST_PASSED" = "true" ] && echo "âœ… PASSED" || echo "âŒ FAILED")
          **Passed:** $TEST_SUCCESS_COUNT tests
          **Total:** $TEST_TOTAL_COUNT tests
          **Exit Code:** $TEST_EXIT_CODE
          **Generated:** $(date -u)

          ## Files Available
          - [test-output.log](./test-output.log) - Complete test execution log
          - [test-results.json](./test-results.json) - JSON test results (if generated)
          - [test-failures-detailed.log](./test-failures-detailed.log) - Detailed failure analysis (if failures occurred)

          $([ "$TEST_PASSED" != "true" ] && cat << TESTEOF

          ## âŒ Test Failures Detected
          Tests failed during execution. Check the detailed logs above for specific error messages and stack traces.

          TESTEOF
          )
          EOF

          echo "âœ… Test reports prepared for inclusion in final reports"

      - name: "ðŸ”¦ Run Lighthouse on authenticated pages"
        continue-on-error: true
        run: |
          # For now, only targeting dev environment
          TARGET_URL="${{ github.event.inputs.target_url || 'https://staging.violetvault.app' }}"

          # Initialize failure tracking
          LIGHTHOUSE_FAILURES=""
          SUCCESS_COUNT=0
          TOTAL_COUNT=0

          # Pages to test (authenticated)
          PAGES=(
            "/dashboard"
            "/envelopes"
            "/transactions"
            "/bills"
            "/savings"
            "/analytics"
          )

          for PAGE in "${PAGES[@]}"; do
            echo "ðŸ”¦ Testing $TARGET_URL$PAGE"
            TOTAL_COUNT=$((TOTAL_COUNT + 1))

            if npx --yes lighthouse "$TARGET_URL$PAGE" \
              --chrome-flags="--headless --no-sandbox --disable-gpu" \
              --output=html \
              --output=json \
              --output-path="./lighthouse-results/$(echo $PAGE | sed 's/\//-/g')" \
              --preset=desktop \
              --quiet; then
              echo "âœ… Lighthouse succeeded for $PAGE"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            else
              echo "âš ï¸ Lighthouse failed for $PAGE"
              LIGHTHOUSE_FAILURES="$LIGHTHOUSE_FAILURES$PAGE "
            fi
          done

          # Store results for later steps
          echo "SUCCESS_COUNT=$SUCCESS_COUNT" >> $GITHUB_ENV
          echo "TOTAL_COUNT=$TOTAL_COUNT" >> $GITHUB_ENV
          echo "LIGHTHOUSE_FAILURES=$LIGHTHOUSE_FAILURES" >> $GITHUB_ENV

      - name: "ðŸ“Š Generate summary report"
        run: |
          echo "# ðŸ”¦ Lighthouse Performance Report" > lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md
          echo "**Generated:** $(date -u)" >> lighthouse-results/SUMMARY.md
          echo "**Target:** ${{ github.event.inputs.target_url || 'https://staging.violetvault.app' }}" >> lighthouse-results/SUMMARY.md
          echo "**Branch:** ${{ github.ref_name }}" >> lighthouse-results/SUMMARY.md
          echo "**Commit:** ${{ github.sha }}" >> lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md

          # Add test results summary with links to detailed reports
          echo "## ðŸ§ª Test Results" >> lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md
          if [ "$TEST_PASSED" = "true" ]; then
            echo "âœ… **All tests passed** ($TEST_SUCCESS_COUNT/$TEST_TOTAL_COUNT)" >> lighthouse-results/SUMMARY.md
          else
            echo "âŒ **Some tests failed** ($TEST_SUCCESS_COUNT/$TEST_TOTAL_COUNT)" >> lighthouse-results/SUMMARY.md
            echo "" >> lighthouse-results/SUMMARY.md
            echo "### ðŸ“‹ Test Reports Available:" >> lighthouse-results/SUMMARY.md
            echo "- [ðŸ“„ Complete Test Output](./test-output.log)" >> lighthouse-results/SUMMARY.md
            echo "- [ðŸ“Š Test Summary](./test-summary.md)" >> lighthouse-results/SUMMARY.md
            echo "- [ðŸ” Detailed Failure Analysis](./test-failures-detailed.log)" >> lighthouse-results/SUMMARY.md
            if [ -f "lighthouse-results/test-results.json" ]; then
              echo "- [ðŸ“„ JSON Test Results](./test-results.json)" >> lighthouse-results/SUMMARY.md
            fi
            echo "" >> lighthouse-results/SUMMARY.md
            echo "### Quick Failure Summary:" >> lighthouse-results/SUMMARY.md
            echo '```' >> lighthouse-results/SUMMARY.md
            echo "$TEST_FAILURES" | head -20 >> lighthouse-results/SUMMARY.md
            echo '```' >> lighthouse-results/SUMMARY.md
            echo "" >> lighthouse-results/SUMMARY.md
            echo "*See detailed reports above for complete failure information*" >> lighthouse-results/SUMMARY.md
          fi
          echo "" >> lighthouse-results/SUMMARY.md

          # Add lighthouse results summary
          echo "## ðŸ”¦ Lighthouse Results" >> lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md

          # Parse JSON results and create summary
          for file in lighthouse-results/*.json; do
            if [ -f "$file" ]; then
              PAGE=$(basename "$file" .json)
              PERFORMANCE=$(jq -r '.categories.performance.score * 100 | floor' "$file" 2>/dev/null || echo "N/A")
              ACCESSIBILITY=$(jq -r '.categories.accessibility.score * 100 | floor' "$file" 2>/dev/null || echo "N/A")
              BEST_PRACTICES=$(jq -r '.categories["best-practices"].score * 100 | floor' "$file" 2>/dev/null || echo "N/A")
              SEO=$(jq -r '.categories.seo.score * 100 | floor' "$file" 2>/dev/null || echo "N/A")

              echo "## ðŸ“„ Page: $PAGE" >> lighthouse-results/SUMMARY.md
              echo "| Metric | Score |" >> lighthouse-results/SUMMARY.md
              echo "|--------|-------|" >> lighthouse-results/SUMMARY.md
              echo "| ðŸš€ Performance | $PERFORMANCE% |" >> lighthouse-results/SUMMARY.md
              echo "| â™¿ Accessibility | $ACCESSIBILITY% |" >> lighthouse-results/SUMMARY.md
              echo "| âœ… Best Practices | $BEST_PRACTICES% |" >> lighthouse-results/SUMMARY.md
              echo "| ðŸ” SEO | $SEO% |" >> lighthouse-results/SUMMARY.md
              echo "" >> lighthouse-results/SUMMARY.md
            fi
          done

      - name: "ðŸ“¤ Upload Lighthouse reports"
        uses: actions/upload-artifact@v6
        with:
          name: lighthouse-reports-${{ github.run_number }}
          path: lighthouse-results/
          retention-days: 30

      - name: "ðŸ“‹ Push reports to lighthouse-reports branch"
        run: |
          # Create timestamped directory for this run
          TIMESTAMP=$(date -u +"%Y-%m-%d_%H-%M-%S")
          BRANCH_NAME="lighthouse-reports"
          REPORT_DIR="reports/${{ github.ref_name }}/${TIMESTAMP}"

          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Create or checkout lighthouse-reports branch
          git fetch origin $BRANCH_NAME:$BRANCH_NAME 2>/dev/null || git checkout --orphan $BRANCH_NAME
          git checkout $BRANCH_NAME 2>/dev/null || true

          # Clear working directory but keep .git
          find . -mindepth 1 -maxdepth 1 ! -name '.git' -exec rm -rf {} \; 2>/dev/null || true

          # Create report directory structure
          mkdir -p "$REPORT_DIR"

          # Archive old reports (older than 7 days)
          echo "ðŸ—„ï¸ Archiving reports older than 7 days..."
          CUTOFF_DATE=$(date -u -d '7 days ago' '+%Y-%m-%d_%H-%M-%S' 2>/dev/null || date -u -v-7d '+%Y-%m-%d_%H-%M-%S' 2>/dev/null || echo "")

          if [ -n "$CUTOFF_DATE" ]; then
            # Find and archive old report directories
            for branch_dir in reports/*/; do
              if [ -d "$branch_dir" ]; then
                branch_name=$(basename "$branch_dir")
                echo "Checking branch: $branch_name"

                # Count total reports before cleanup
                TOTAL_REPORTS=$(find "$branch_dir" -type d -name "20*" | wc -l || echo "0")

                # Find old directories to archive
                OLD_DIRS=$(find "$branch_dir" -type d -name "20*" | while read dir; do
                  dir_date=$(basename "$dir")
                  if [ "$dir_date" \< "$CUTOFF_DATE" ]; then
                    echo "$dir"
                  fi
                done)

                if [ -n "$OLD_DIRS" ]; then
                  echo "Archiving old reports for $branch_name..."

                  # Create archive directory
                  ARCHIVE_DIR="archived/$branch_name"
                  mkdir -p "$ARCHIVE_DIR"

                  # Move old directories to archive
                  echo "$OLD_DIRS" | while read old_dir; do
                    if [ -d "$old_dir" ]; then
                      echo "  Archiving: $(basename "$old_dir")"
                      mv "$old_dir" "$ARCHIVE_DIR/"
                    fi
                  done

                  # Count remaining reports
                  REMAINING_REPORTS=$(find "$branch_dir" -type d -name "20*" | wc -l || echo "0")
                  ARCHIVED_COUNT=$((TOTAL_REPORTS - REMAINING_REPORTS))

                  echo "  âœ… Archived $ARCHIVED_COUNT reports, keeping $REMAINING_REPORTS recent reports"
                else
                  echo "  âœ… No old reports to archive for $branch_name"
                fi
              fi
            done
          else
            echo "  âš ï¸ Could not determine cutoff date for archiving"
          fi

          # Copy reports to timestamped directory (if any exist)
          if [ "$(ls -A lighthouse-results/ 2>/dev/null)" ]; then
            cp -r lighthouse-results/* "$REPORT_DIR/"
          else
            echo "âš ï¸ No lighthouse results found to copy"
          fi

          # Create failure report if there were any failures (lighthouse or tests)
          if [ "$SUCCESS_COUNT" -lt "$TOTAL_COUNT" ] || [ "$TEST_PASSED" != "true" ]; then
            cat > "$REPORT_DIR/test-and-lighthouse-failures.md" << EOF
          # ðŸš¨ Test & Lighthouse Failure Report

          **Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          **Branch:** ${{ github.ref_name }}
          **Target URL:** ${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}
          **Node.js Version:** $(node --version)
          **Workflow Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ## ðŸ§ª Test Results Summary
          - **Total Tests:** $TEST_TOTAL_COUNT
          - **Passed Tests:** $TEST_SUCCESS_COUNT
          - **Failed Tests:** $((TEST_TOTAL_COUNT - TEST_SUCCESS_COUNT))
          - **Test Status:** $([ "$TEST_PASSED" = "true" ] && echo "âœ… PASSED" || echo "âŒ FAILED")

          $([ "$TEST_PASSED" != "true" ] && cat << TESTEOF

          ### âŒ Test Failures
          \`\`\`
          $TEST_FAILURES
          \`\`\`
          TESTEOF
          )

          ## ðŸ”¦ Lighthouse Results Summary
          - **Total Pages Tested:** $TOTAL_COUNT
          - **Successful Scans:** $SUCCESS_COUNT
          - **Failed Scans:** $((TOTAL_COUNT - SUCCESS_COUNT))
          - **Lighthouse Status:** $([ "$SUCCESS_COUNT" -eq "$TOTAL_COUNT" ] && echo "âœ… PASSED" || echo "âŒ FAILED")

          $([ "$SUCCESS_COUNT" -lt "$TOTAL_COUNT" ] && cat << LHEOF

          ### âŒ Failed Lighthouse Pages
          $LIGHTHOUSE_FAILURES
          LHEOF
          )

          ## ðŸ” Possible Causes

          ### Test Failures
          1. **Code Changes**: Recent code changes may have broken existing functionality
          2. **Dependency Issues**: Package updates or missing dependencies
          3. **Environment Issues**: Mock setup or test environment configuration
          4. **API Changes**: Breaking changes in external APIs or internal interfaces

          ### Lighthouse Failures
          1. **Node.js Compatibility**: Check if Node.js version supports all dependencies
          2. **Authentication Issues**: Dev auth token may have expired or be invalid
          3. **Network Connectivity**: Target URL may be unreachable or slow
          4. **Page Load Errors**: JavaScript errors preventing page load
          5. **Lighthouse Installation**: Lighthouse CLI may not be properly installed
          6. **Chrome Dependencies**: Headless Chrome may be missing required dependencies

          ## ðŸ› ï¸ Debugging Steps

          ### For Test Failures
          1. Run tests locally: \`npm run test\`
          2. Check for recent code changes that might affect tests
          3. Verify all mocks and test setup are correct
          4. Review test output for specific error messages

          ### For Lighthouse Failures
          1. Check workflow logs for specific error messages
          2. Verify target URL is accessible
          3. Test authentication flow manually
          4. Check for JavaScript console errors on target pages
          5. Review Node.js and dependency versions

          ---
          *This report was automatically generated by the enhanced Lighthouse + Test monitoring workflow*
          EOF
          fi

          # Create/update index with latest reports
          cat > README.md << 'EOF'
          # ðŸ”¦ Lighthouse Performance Reports

          Automated performance monitoring reports for Violet Vault.

          **ðŸ“‹ Retention Policy:** Reports are automatically archived after 7 days to keep this branch manageable.

          ## Latest Reports

          EOF

          # Add archiving summary if archives exist
          if [ -d "archived" ] && [ "$(ls -A archived/ 2>/dev/null)" ]; then
            echo "" >> README.md
            echo "## ðŸ—„ï¸ Archived Reports" >> README.md
            echo "" >> README.md
            echo "Older reports (>7 days) are archived to maintain performance:" >> README.md
            echo "" >> README.md

            for archive_branch in archived/*/; do
              if [ -d "$archive_branch" ]; then
                branch_name=$(basename "$archive_branch")
                archive_count=$(find "$archive_branch" -type d -name "20*" | wc -l || echo "0")
                if [ "$archive_count" -gt 0 ]; then
                  echo "- **$branch_name**: $archive_count archived reports" >> README.md
                fi
              fi
            done
            echo "" >> README.md
          fi

          # Add this run to the index
          echo "### $(date -u '+%Y-%m-%d %H:%M:%S UTC') - ${{ github.ref_name }}" >> README.md
          echo "" >> README.md
          echo "**Commit:** [\`${{ github.sha }}\`](${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }})" >> README.md
          echo "**Workflow:** [View Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> README.md
          echo "**Reports:** [Browse Reports](${{ github.server_url }}/${{ github.repository }}/tree/lighthouse-reports/$REPORT_DIR)" >> README.md
          echo "" >> README.md

          # Add report links
          if [ -f "$REPORT_DIR/test-and-lighthouse-failures.md" ]; then
            echo "**âŒ [View Test & Lighthouse Failure Report](./$REPORT_DIR/test-and-lighthouse-failures.md)**" >> README.md
            echo "" >> README.md
          fi

          # Add test reports section
          if [ -f "$REPORT_DIR/test-summary.md" ]; then
            echo "#### ðŸ§ª Test Reports" >> README.md
            echo "- [ðŸ“Š Test Summary](./$REPORT_DIR/test-summary.md)" >> README.md
            if [ -f "$REPORT_DIR/test-output.log" ]; then
              echo "- [ðŸ“„ Complete Test Log](./$REPORT_DIR/test-output.log)" >> README.md
            fi
            if [ -f "$REPORT_DIR/test-failures-detailed.log" ]; then
              echo "- [ðŸ” Detailed Failure Analysis](./$REPORT_DIR/test-failures-detailed.log)" >> README.md
            fi
            echo "" >> README.md
          fi

          # Add lighthouse reports section
          if [ "$(ls "$REPORT_DIR"/*.html 2>/dev/null)" ]; then
            echo "#### ðŸ”¦ Lighthouse Reports" >> README.md
            for file in "$REPORT_DIR"/*.html; do
              if [ -f "$file" ]; then
                PAGE=$(basename "$file" .report.html)
                echo "- [${PAGE} Report](./$REPORT_DIR/$(basename "$file"))" >> README.md
              fi
            done
            echo "" >> README.md
          fi

          # Add main summary report
          if [ -f "$REPORT_DIR/SUMMARY.md" ]; then
            echo "#### ðŸ“‹ [Complete Summary Report](./$REPORT_DIR/SUMMARY.md)" >> README.md
            echo "" >> README.md
          fi

          echo "" >> README.md
          echo "---" >> README.md
          echo "" >> README.md

          # Add all changes
          git add .

          # Commit if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "ðŸ“Š Lighthouse reports for ${{ github.ref_name }} - $(date -u '+%Y-%m-%d %H:%M:%S UTC')

          Generated from commit ${{ github.sha }}
          Workflow run: ${{ github.run_id }}"

            git push origin $BRANCH_NAME
            echo "âœ… Reports pushed to $BRANCH_NAME branch"
          else
            echo "â„¹ï¸ No changes to commit"
          fi

      - name: "ðŸ’¬ Comment results on latest commit (if on PR)"
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('lighthouse-results/SUMMARY.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ðŸ”¦ Lighthouse Performance Report\n\n${summary}\n\nðŸ“Š [View detailed reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`
            });

      - name: "ðŸš¨ Check for performance regressions and test failures"
        run: |
          FAILED_PAGES=""
          LIGHTHOUSE_ISSUES=false
          OVERALL_ISSUES=false

          # Check lighthouse results
          for file in lighthouse-results/*.json; do
            if [ -f "$file" ]; then
              PAGE=$(basename "$file" .json)
              PERFORMANCE=$(jq -r '.categories.performance.score * 100 | floor' "$file" 2>/dev/null || echo "0")

              # Get all category scores
              ACCESSIBILITY=$(jq -r '.categories.accessibility.score * 100 | floor' "$file" 2>/dev/null || echo "0")
              BEST_PRACTICES=$(jq -r '.categories."best-practices".score * 100 | floor' "$file" 2>/dev/null || echo "0")
              SEO=$(jq -r '.categories.seo.score * 100 | floor' "$file" 2>/dev/null || echo "0")

              # Alert if any category is not 100%
              ISSUES=""
              if [ "$PERFORMANCE" -lt 100 ] && [ "$PERFORMANCE" != "N/A" ]; then
                ISSUES="$ISSUES Performance: $PERFORMANCE%"
              fi
              if [ "$ACCESSIBILITY" -lt 100 ] && [ "$ACCESSIBILITY" != "N/A" ]; then
                ISSUES="$ISSUES${ISSUES:+, }Accessibility: $ACCESSIBILITY%"
              fi
              if [ "$BEST_PRACTICES" -lt 100 ] && [ "$BEST_PRACTICES" != "N/A" ]; then
                ISSUES="$ISSUES${ISSUES:+, }Best Practices: $BEST_PRACTICES%"
              fi
              if [ "$SEO" -lt 100 ] && [ "$SEO" != "N/A" ]; then
                ISSUES="$ISSUES${ISSUES:+, }SEO: $SEO%"
              fi

              if [ -n "$ISSUES" ]; then
                FAILED_PAGES="$FAILED_PAGES\n- $PAGE: $ISSUES"
                LIGHTHOUSE_ISSUES=true
              fi
            fi
          done

          # Check for any issues (lighthouse or tests)
          if [ "$LIGHTHOUSE_ISSUES" = "true" ] || [ "$TEST_PASSED" != "true" ]; then
            OVERALL_ISSUES=true
            echo "ðŸš¨ Issues detected:"
            if [ "$TEST_PASSED" != "true" ]; then
              echo "  - âŒ Test failures: $((TEST_TOTAL_COUNT - TEST_SUCCESS_COUNT))/$TEST_TOTAL_COUNT tests failed"
            fi
            if [ "$LIGHTHOUSE_ISSUES" = "true" ]; then
              echo "  - âŒ Lighthouse score issues:"
              echo -e "$FAILED_PAGES"
            fi
          else
            echo "âœ… All tests pass and all pages achieve 100% lighthouse scores"
          fi

          # Set outputs
          echo "overall_issues=$OVERALL_ISSUES" >> $GITHUB_OUTPUT
          echo "lighthouse_issues=$LIGHTHOUSE_ISSUES" >> $GITHUB_OUTPUT
          echo "test_failures=$([[ "$TEST_PASSED" != "true" ]] && echo true || echo false)" >> $GITHUB_OUTPUT
          echo "failed_pages<<EOF" >> $GITHUB_OUTPUT
          echo -e "$FAILED_PAGES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        id: check_issues

      - name: "ðŸ“§ Create issue for test/lighthouse failures"
        if: steps.check_issues.outputs.overall_issues == 'true'
        uses: actions/github-script@v8
        env:
          FAILED_PAGES: ${{ steps.check_issues.outputs.failed_pages }}
          TARGET_URL: ${{ github.event.inputs.target_url || 'https://staging.violetvault.app' }}
          BRANCH_NAME: ${{ github.ref_name }}
          LIGHTHOUSE_ISSUES: ${{ steps.check_issues.outputs.lighthouse_issues }}
          TEST_FAILURES: ${{ steps.check_issues.outputs.test_failures }}
          TEST_PASSED: ${{ env.TEST_PASSED }}
          TEST_SUCCESS_COUNT: ${{ env.TEST_SUCCESS_COUNT }}
          TEST_TOTAL_COUNT: ${{ env.TEST_TOTAL_COUNT }}
        with:
          script: |
            const failedPages = process.env.FAILED_PAGES;
            const targetUrl = process.env.TARGET_URL;
            const branchName = process.env.BRANCH_NAME;
            const hasLighthouseIssues = process.env.LIGHTHOUSE_ISSUES === 'true';
            const hasTestFailures = process.env.TEST_FAILURES === 'true';
            const testPassed = process.env.TEST_PASSED === 'true';
            const testSuccessCount = process.env.TEST_SUCCESS_COUNT || '0';
            const testTotalCount = process.env.TEST_TOTAL_COUNT || '0';

            // Determine branch-specific label and issue type
            const branchLabel = branchName === 'develop' ? 'dev branch' : 'production-build';
            const issueType = hasTestFailures && hasLighthouseIssues ? 'test-lighthouse' :
                            hasTestFailures ? 'test' : 'lighthouse';

            // Check for existing open issues for this specific branch and type
            const searchLabels = hasTestFailures ?
              `monitoring,${branchLabel},test` :
              `monitoring,${branchLabel},lighthouse`;

            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: searchLabels,
              per_page: 5
            });

            if (existingIssues.data.length > 0) {
              const existingIssue = existingIssues.data[0];
              console.log(`âœ… Skipping duplicate issue - open ${issueType} issue exists for ${branchName}: #${existingIssue.number}`);

              // Update existing issue with latest report link
              const updateBody = `\n\n---\n**Latest Report Update:** ${new Date().toISOString()}\nðŸ“Š [View latest reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`;

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: updateBody
              });

              return;
            }

            // Build issue title and body based on failure type
            let issueTitle;
            let issueBody = [];

            if (hasTestFailures && hasLighthouseIssues) {
              issueTitle = `ðŸš¨ Test & Lighthouse Failures - ${branchName} - ${new Date().toISOString().split('T')[0]}`;
              issueBody = [
                "## ðŸš¨ Test & Lighthouse Failure Alert",
                "",
                `**Date:** ${new Date().toISOString()}`,
                `**Target:** ${targetUrl}`,
                `**Branch:** ${branchName}`,
                "",
                "### ðŸ§ª Test Results",
                `- **Status:** âŒ FAILED`,
                `- **Passed:** ${testSuccessCount}/${testTotalCount}`,
                `- **Failed:** ${testTotalCount - testSuccessCount}`,
                "",
                "### ðŸ”¦ Lighthouse Results",
                "ðŸš¨ Pages Not Achieving 100% Scores:",
                failedPages
              ];
            } else if (hasTestFailures) {
              issueTitle = `ðŸ§ª Test Failures - ${branchName} - ${new Date().toISOString().split('T')[0]}`;
              issueBody = [
                "## ðŸ§ª Test Failure Alert",
                "",
                `**Date:** ${new Date().toISOString()}`,
                `**Target:** ${targetUrl}`,
                `**Branch:** ${branchName}`,
                "",
                "### ðŸš¨ Test Results",
                `- **Status:** âŒ FAILED`,
                `- **Passed:** ${testSuccessCount}/${testTotalCount}`,
                `- **Failed:** ${testTotalCount - testSuccessCount}`
              ];
            } else {
              issueTitle = `ðŸ”¦ Lighthouse Optimization Needed - ${branchName} - ${new Date().toISOString().split('T')[0]}`;
              issueBody = [
                "## ðŸ”¦ Lighthouse Score Optimization Alert",
                "",
                `**Date:** ${new Date().toISOString()}`,
                `**Target:** ${targetUrl}`,
                `**Branch:** ${branchName}`,
                "",
                "### ðŸš¨ Pages Not Achieving 100% Scores:",
                failedPages
              ];
            }

            // Common sections
            issueBody.push(
              "",
              "### ðŸ“Š Full Report",
              `[View complete test & lighthouse reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`,
              "",
              "### ðŸ”§ Next Steps"
            );

            if (hasTestFailures) {
              issueBody.push(
                "#### For Test Failures:",
                "1. Run tests locally: `npm run test`",
                "2. Review failed test output for specific error messages",
                "3. Check for recent code changes that might affect tests",
                "4. Verify all mocks and test setup are correct",
                ""
              );
            }

            if (hasLighthouseIssues) {
              issueBody.push(
                "#### For Lighthouse Issues:",
                "1. Review the detailed Lighthouse reports",
                "2. Identify specific optimization opportunities",
                "3. Fix accessibility, performance, SEO, or best practice issues",
                "4. Re-run monitoring to verify 100% scores",
                ""
              );
            }

            issueBody.push(
              "### ðŸŽ¯ Goals",
              hasTestFailures ? "- âœ… All tests must pass" : "",
              hasLighthouseIssues ? "- ðŸ”¦ Achieve 100% scores across all Lighthouse categories:" : "",
              hasLighthouseIssues ? "  - Performance: 100%" : "",
              hasLighthouseIssues ? "  - Accessibility: 100%" : "",
              hasLighthouseIssues ? "  - Best Practices: 100%" : "",
              hasLighthouseIssues ? "  - SEO: 100%" : "",
              "",
              "---",
              "*This issue was automatically created by the enhanced Test + Lighthouse monitoring workflow*"
            );

            // Set appropriate labels
            const labels = ['monitoring', branchLabel];
            if (hasTestFailures) labels.push('test', 'bug');
            if (hasLighthouseIssues) labels.push('lighthouse', 'performance');

            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: issueTitle,
              body: issueBody.filter(line => line !== "").join('\n'),
              labels: labels
            });

      - name: "ðŸ“‹ Final Workflow Status"
        run: |
          echo "## ðŸ”¦ Lighthouse Monitoring Workflow Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** ${{ github.event.inputs.target_url || 'https://staging.violetvault.app' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Time:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Test Results Summary
          echo "### ðŸ§ª Test Results" >> $GITHUB_STEP_SUMMARY
          if [ "$TEST_PASSED" = "true" ]; then
            echo "âœ… **Tests:** All $TEST_SUCCESS_COUNT tests passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Tests:** $((TEST_TOTAL_COUNT - TEST_SUCCESS_COUNT))/$TEST_TOTAL_COUNT tests failed (Exit code: $TEST_EXIT_CODE)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Lighthouse Results Summary
          echo "### ðŸ”¦ Lighthouse Results" >> $GITHUB_STEP_SUMMARY
          if [ "$SUCCESS_COUNT" -eq "$TOTAL_COUNT" ] && [ "$TOTAL_COUNT" -gt 0 ]; then
            echo "âœ… **Lighthouse:** All $SUCCESS_COUNT/$TOTAL_COUNT pages scanned successfully" >> $GITHUB_STEP_SUMMARY
          elif [ "$TOTAL_COUNT" -gt 0 ]; then
            echo "âš ï¸ **Lighthouse:** $SUCCESS_COUNT/$TOTAL_COUNT pages scanned successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Lighthouse:** No pages were scanned" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall Status
          echo "### ðŸ“Š Overall Status" >> $GITHUB_STEP_SUMMARY
          if [ "$SUCCESS_COUNT" -eq "$TOTAL_COUNT" ] && [ "$TOTAL_COUNT" -gt 0 ]; then
            echo "âœ… **Lighthouse monitoring completed successfully**" >> $GITHUB_STEP_SUMMARY
            if [ "$TEST_PASSED" != "true" ]; then
              echo "âš ï¸ Note: Test failures detected but lighthouse tests completed successfully" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âŒ **Lighthouse monitoring failed**" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š [View detailed reports](${{ github.server_url }}/${{ github.repository }}/tree/lighthouse-reports)" >> $GITHUB_STEP_SUMMARY

          # Only fail the workflow if lighthouse completely failed
          if [ "$TOTAL_COUNT" -eq 0 ]; then
            echo "ðŸš¨ No lighthouse tests were executed - failing workflow"
            exit 1
          elif [ "$SUCCESS_COUNT" -lt "$TOTAL_COUNT" ]; then
            echo "âš ï¸ Some lighthouse tests failed - this is informational only"
            echo "The workflow will continue to pass as lighthouse partially succeeded"
          else
            echo "âœ… All lighthouse tests passed - workflow succeeds"
          fi

  # Send notification summary
  notify:
    needs: lighthouse-audit
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main')

    steps:
      - name: "ðŸ“Š Workflow Summary"
        run: |
          # For now, only targeting dev environment
          TARGET_URL="${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}"

          echo "## ðŸ”¦ Lighthouse Monitoring Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ needs.lighthouse-audit.result }}" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** $TARGET_URL" >> $GITHUB_STEP_SUMMARY
          echo "**Time:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š [View Reports](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
