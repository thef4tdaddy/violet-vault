name: "🔦 Lighthouse Performance Monitoring"

on:
  # Run every 2 hours on develop
  schedule:
    - cron: "0 */2 * * *" # Every 2 hours for develop
    # TODO: Enable main branch monitoring when production has dev auth bypass
    # - cron: "0 6 * * *"    # Daily at 6 AM for main

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      target_url:
        description: "Target URL to test"
        required: false
        default: "https://dev.f4tdaddy.com"
        type: string

jobs:
  lighthouse-audit:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main' # Run on both develop and main branches

    steps:
      - name: "🏗️ Checkout code"
        uses: actions/checkout@v4

      - name: "🔧 Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: "📦 Install dependencies"
        run: npm ci

      - name: "🧪 Run test suite"
        run: |
          echo "🧪 Running Vitest test suite..."

          # Initialize test results tracking
          TEST_FAILURES=""
          TEST_SUCCESS_COUNT=0
          TEST_TOTAL_COUNT=0
          TEST_PASSED=true

          # Run tests and capture results
          if npm run test 2>&1 | tee test-output.log; then
            echo "✅ All tests passed"
            TEST_SUCCESS_COUNT=$(grep -o "passed" test-output.log | wc -l || echo "0")
            TEST_TOTAL_COUNT=$TEST_SUCCESS_COUNT
          else
            echo "❌ Some tests failed"
            TEST_PASSED=false

            # Extract test statistics from output
            TEST_TOTAL_COUNT=$(grep -E "Tests:|Test Files:" test-output.log | head -1 | grep -o '[0-9]\+' | head -1 || echo "0")
            TEST_SUCCESS_COUNT=$(grep -o "passed" test-output.log | wc -l || echo "0")

            # Capture failed test information
            TEST_FAILURES=$(grep -A 5 -B 5 "FAIL\|✕\|failed" test-output.log || echo "Test failures detected but details not captured")
          fi

          # Store results for later steps
          echo "TEST_PASSED=$TEST_PASSED" >> $GITHUB_ENV
          echo "TEST_SUCCESS_COUNT=$TEST_SUCCESS_COUNT" >> $GITHUB_ENV
          echo "TEST_TOTAL_COUNT=$TEST_TOTAL_COUNT" >> $GITHUB_ENV
          echo "TEST_FAILURES<<EOF" >> $GITHUB_ENV
          echo "$TEST_FAILURES" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV

      - name: "🎫 Generate dev auth token"
        id: auth
        run: |
          TOKEN=$(DEV_AUTH_SECRET="${{ secrets.DEV_AUTH_SECRET }}" node scripts/generate-dev-auth-token.js | grep -A1 "🔗 Token:" | tail -1)
          echo "token=$TOKEN" >> $GITHUB_OUTPUT
        env:
          DEV_AUTH_SECRET: ${{ secrets.DEV_AUTH_SECRET }}

      - name: "🔐 Authenticate with dev auth bypass"
        run: |
          # For now, only targeting dev environment
          TARGET_URL="${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}"
          echo "🔐 Authenticating with $TARGET_URL..."

          # Authenticate and get cookies
          curl -c cookies.txt -b cookies.txt -L "$TARGET_URL/__dev_auth?token=${{ steps.auth.outputs.token }}&target=/dashboard"

          echo "✅ Authentication complete"

      - name: "📂 Create lighthouse directory"
        run: mkdir -p lighthouse-results

      - name: "🔦 Run Lighthouse on authenticated pages"
        run: |
          # For now, only targeting dev environment
          TARGET_URL="${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}"

          # Initialize failure tracking
          LIGHTHOUSE_FAILURES=""
          SUCCESS_COUNT=0
          TOTAL_COUNT=0

          # Pages to test (authenticated)
          PAGES=(
            "/dashboard"
            "/envelopes"
            "/transactions"
            "/bills"
            "/savings"
            "/analytics"
          )

          for PAGE in "${PAGES[@]}"; do
            echo "🔦 Testing $TARGET_URL$PAGE"
            TOTAL_COUNT=$((TOTAL_COUNT + 1))

            if npx lighthouse "$TARGET_URL$PAGE" \
              --chrome-flags="--headless --no-sandbox --disable-gpu" \
              --output=html \
              --output=json \
              --output-path="./lighthouse-results/$(echo $PAGE | sed 's/\//-/g')" \
              --preset=desktop \
              --quiet; then
              echo "✅ Lighthouse succeeded for $PAGE"
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
            else
              echo "⚠️ Lighthouse failed for $PAGE"
              LIGHTHOUSE_FAILURES="$LIGHTHOUSE_FAILURES$PAGE "
            fi
          done

          # Store results for later steps
          echo "SUCCESS_COUNT=$SUCCESS_COUNT" >> $GITHUB_ENV
          echo "TOTAL_COUNT=$TOTAL_COUNT" >> $GITHUB_ENV
          echo "LIGHTHOUSE_FAILURES=$LIGHTHOUSE_FAILURES" >> $GITHUB_ENV

      - name: "📊 Generate summary report"
        run: |
          echo "# 🔦 Lighthouse Performance Report" > lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md
          echo "**Generated:** $(date -u)" >> lighthouse-results/SUMMARY.md
          echo "**Target:** ${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}" >> lighthouse-results/SUMMARY.md
          echo "**Branch:** ${{ github.ref_name }}" >> lighthouse-results/SUMMARY.md
          echo "**Commit:** ${{ github.sha }}" >> lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md

          # Add test results summary
          echo "## 🧪 Test Results" >> lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md
          if [ "$TEST_PASSED" = "true" ]; then
            echo "✅ **All tests passed** ($TEST_SUCCESS_COUNT/$TEST_TOTAL_COUNT)" >> lighthouse-results/SUMMARY.md
          else
            echo "❌ **Some tests failed** ($TEST_SUCCESS_COUNT/$TEST_TOTAL_COUNT)" >> lighthouse-results/SUMMARY.md
            echo "" >> lighthouse-results/SUMMARY.md
            echo "### Failed Tests:" >> lighthouse-results/SUMMARY.md
            echo '```' >> lighthouse-results/SUMMARY.md
            echo "$TEST_FAILURES" >> lighthouse-results/SUMMARY.md
            echo '```' >> lighthouse-results/SUMMARY.md
          fi
          echo "" >> lighthouse-results/SUMMARY.md

          # Add lighthouse results summary
          echo "## 🔦 Lighthouse Results" >> lighthouse-results/SUMMARY.md
          echo "" >> lighthouse-results/SUMMARY.md

          # Parse JSON results and create summary
          for file in lighthouse-results/*.json; do
            if [ -f "$file" ]; then
              PAGE=$(basename "$file" .json)
              PERFORMANCE=$(jq -r '.categories.performance.score * 100 | floor' "$file" 2>/dev/null || echo "N/A")
              ACCESSIBILITY=$(jq -r '.categories.accessibility.score * 100 | floor' "$file" 2>/dev/null || echo "N/A")
              BEST_PRACTICES=$(jq -r '.categories["best-practices"].score * 100 | floor' "$file" 2>/dev/null || echo "N/A")
              SEO=$(jq -r '.categories.seo.score * 100 | floor' "$file" 2>/dev/null || echo "N/A")

              echo "## 📄 Page: $PAGE" >> lighthouse-results/SUMMARY.md
              echo "| Metric | Score |" >> lighthouse-results/SUMMARY.md
              echo "|--------|-------|" >> lighthouse-results/SUMMARY.md
              echo "| 🚀 Performance | $PERFORMANCE% |" >> lighthouse-results/SUMMARY.md
              echo "| ♿ Accessibility | $ACCESSIBILITY% |" >> lighthouse-results/SUMMARY.md
              echo "| ✅ Best Practices | $BEST_PRACTICES% |" >> lighthouse-results/SUMMARY.md
              echo "| 🔍 SEO | $SEO% |" >> lighthouse-results/SUMMARY.md
              echo "" >> lighthouse-results/SUMMARY.md
            fi
          done

      - name: "📤 Upload Lighthouse reports"
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-reports-${{ github.run_number }}
          path: lighthouse-results/
          retention-days: 30

      - name: "📋 Push reports to lighthouse-reports branch"
        run: |
          # Create timestamped directory for this run
          TIMESTAMP=$(date -u +"%Y-%m-%d_%H-%M-%S")
          BRANCH_NAME="lighthouse-reports"
          REPORT_DIR="reports/${{ github.ref_name }}/${TIMESTAMP}"

          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Create or checkout lighthouse-reports branch
          git fetch origin $BRANCH_NAME:$BRANCH_NAME 2>/dev/null || git checkout --orphan $BRANCH_NAME
          git checkout $BRANCH_NAME 2>/dev/null || true

          # Clear working directory but keep .git
          find . -mindepth 1 -maxdepth 1 ! -name '.git' -exec rm -rf {} \; 2>/dev/null || true

          # Create report directory structure
          mkdir -p "$REPORT_DIR"

          # Copy reports to timestamped directory (if any exist)
          if [ "$(ls -A lighthouse-results/ 2>/dev/null)" ]; then
            cp -r lighthouse-results/* "$REPORT_DIR/"
          else
            echo "⚠️ No lighthouse results found to copy"
          fi

          # Create failure report if there were any failures (lighthouse or tests)
          if [ "$SUCCESS_COUNT" -lt "$TOTAL_COUNT" ] || [ "$TEST_PASSED" != "true" ]; then
            cat > "$REPORT_DIR/test-and-lighthouse-failures.md" << EOF
          # 🚨 Test & Lighthouse Failure Report

          **Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          **Branch:** ${{ github.ref_name }}
          **Target URL:** ${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}
          **Node.js Version:** $(node --version)
          **Workflow Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ## 🧪 Test Results Summary
          - **Total Tests:** $TEST_TOTAL_COUNT
          - **Passed Tests:** $TEST_SUCCESS_COUNT
          - **Failed Tests:** $((TEST_TOTAL_COUNT - TEST_SUCCESS_COUNT))
          - **Test Status:** $([ "$TEST_PASSED" = "true" ] && echo "✅ PASSED" || echo "❌ FAILED")

          $([ "$TEST_PASSED" != "true" ] && cat << TESTEOF

          ### ❌ Test Failures
          \`\`\`
          $TEST_FAILURES
          \`\`\`
          TESTEOF
          )

          ## 🔦 Lighthouse Results Summary
          - **Total Pages Tested:** $TOTAL_COUNT
          - **Successful Scans:** $SUCCESS_COUNT
          - **Failed Scans:** $((TOTAL_COUNT - SUCCESS_COUNT))
          - **Lighthouse Status:** $([ "$SUCCESS_COUNT" -eq "$TOTAL_COUNT" ] && echo "✅ PASSED" || echo "❌ FAILED")

          $([ "$SUCCESS_COUNT" -lt "$TOTAL_COUNT" ] && cat << LHEOF

          ### ❌ Failed Lighthouse Pages
          $LIGHTHOUSE_FAILURES
          LHEOF
          )

          ## 🔍 Possible Causes

          ### Test Failures
          1. **Code Changes**: Recent code changes may have broken existing functionality
          2. **Dependency Issues**: Package updates or missing dependencies
          3. **Environment Issues**: Mock setup or test environment configuration
          4. **API Changes**: Breaking changes in external APIs or internal interfaces

          ### Lighthouse Failures
          1. **Node.js Compatibility**: Check if Node.js version supports all dependencies
          2. **Authentication Issues**: Dev auth token may have expired or be invalid
          3. **Network Connectivity**: Target URL may be unreachable or slow
          4. **Page Load Errors**: JavaScript errors preventing page load
          5. **Lighthouse Installation**: Lighthouse CLI may not be properly installed
          6. **Chrome Dependencies**: Headless Chrome may be missing required dependencies

          ## 🛠️ Debugging Steps

          ### For Test Failures
          1. Run tests locally: \`npm run test\`
          2. Check for recent code changes that might affect tests
          3. Verify all mocks and test setup are correct
          4. Review test output for specific error messages

          ### For Lighthouse Failures
          1. Check workflow logs for specific error messages
          2. Verify target URL is accessible
          3. Test authentication flow manually
          4. Check for JavaScript console errors on target pages
          5. Review Node.js and dependency versions

          ---
          *This report was automatically generated by the enhanced Lighthouse + Test monitoring workflow*
          EOF
          fi

          # Create/update index with latest reports
          cat > README.md << 'EOF'
          # 🔦 Lighthouse Performance Reports

          Automated performance monitoring reports for Violet Vault.

          ## Latest Reports

          EOF

          # Add this run to the index
          echo "### $(date -u '+%Y-%m-%d %H:%M:%S UTC') - ${{ github.ref_name }}" >> README.md
          echo "" >> README.md
          echo "**Commit:** [\`${{ github.sha }}\`](${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }})" >> README.md
          echo "**Workflow:** [View Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> README.md
          echo "**Reports:** [Browse Reports](${{ github.server_url }}/${{ github.repository }}/tree/lighthouse-reports/$REPORT_DIR)" >> README.md
          echo "" >> README.md

          # Add report links
          if [ -f "$REPORT_DIR/test-and-lighthouse-failures.md" ]; then
            echo "**❌ [View Test & Lighthouse Failure Report](./$REPORT_DIR/test-and-lighthouse-failures.md)**" >> README.md
            echo "" >> README.md
          fi

          for file in "$REPORT_DIR"/*.html; do
            if [ -f "$file" ]; then
              PAGE=$(basename "$file" .report.html)
              echo "- [${PAGE} Report](./$REPORT_DIR/$(basename "$file"))" >> README.md
            fi
          done

          echo "" >> README.md
          echo "---" >> README.md
          echo "" >> README.md

          # Add all changes
          git add .

          # Commit if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "📊 Lighthouse reports for ${{ github.ref_name }} - $(date -u '+%Y-%m-%d %H:%M:%S UTC')

          Generated from commit ${{ github.sha }}
          Workflow run: ${{ github.run_id }}"

            git push origin $BRANCH_NAME
            echo "✅ Reports pushed to $BRANCH_NAME branch"
          else
            echo "ℹ️ No changes to commit"
          fi

      - name: "💬 Comment results on latest commit (if on PR)"
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('lighthouse-results/SUMMARY.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 🔦 Lighthouse Performance Report\n\n${summary}\n\n📊 [View detailed reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`
            });

      - name: "🚨 Check for performance regressions and test failures"
        run: |
          FAILED_PAGES=""
          LIGHTHOUSE_ISSUES=false
          OVERALL_ISSUES=false

          # Check lighthouse results
          for file in lighthouse-results/*.json; do
            if [ -f "$file" ]; then
              PAGE=$(basename "$file" .json)
              PERFORMANCE=$(jq -r '.categories.performance.score * 100 | floor' "$file" 2>/dev/null || echo "0")

              # Get all category scores
              ACCESSIBILITY=$(jq -r '.categories.accessibility.score * 100 | floor' "$file" 2>/dev/null || echo "0")
              BEST_PRACTICES=$(jq -r '.categories."best-practices".score * 100 | floor' "$file" 2>/dev/null || echo "0")
              SEO=$(jq -r '.categories.seo.score * 100 | floor' "$file" 2>/dev/null || echo "0")

              # Alert if any category is not 100%
              ISSUES=""
              if [ "$PERFORMANCE" -lt 100 ] && [ "$PERFORMANCE" != "N/A" ]; then
                ISSUES="$ISSUES Performance: $PERFORMANCE%"
              fi
              if [ "$ACCESSIBILITY" -lt 100 ] && [ "$ACCESSIBILITY" != "N/A" ]; then
                ISSUES="$ISSUES${ISSUES:+, }Accessibility: $ACCESSIBILITY%"
              fi
              if [ "$BEST_PRACTICES" -lt 100 ] && [ "$BEST_PRACTICES" != "N/A" ]; then
                ISSUES="$ISSUES${ISSUES:+, }Best Practices: $BEST_PRACTICES%"
              fi
              if [ "$SEO" -lt 100 ] && [ "$SEO" != "N/A" ]; then
                ISSUES="$ISSUES${ISSUES:+, }SEO: $SEO%"
              fi

              if [ -n "$ISSUES" ]; then
                FAILED_PAGES="$FAILED_PAGES\n- $PAGE: $ISSUES"
                LIGHTHOUSE_ISSUES=true
              fi
            fi
          done

          # Check for any issues (lighthouse or tests)
          if [ "$LIGHTHOUSE_ISSUES" = "true" ] || [ "$TEST_PASSED" != "true" ]; then
            OVERALL_ISSUES=true
            echo "🚨 Issues detected:"
            if [ "$TEST_PASSED" != "true" ]; then
              echo "  - ❌ Test failures: $((TEST_TOTAL_COUNT - TEST_SUCCESS_COUNT))/$TEST_TOTAL_COUNT tests failed"
            fi
            if [ "$LIGHTHOUSE_ISSUES" = "true" ]; then
              echo "  - ❌ Lighthouse score issues:"
              echo -e "$FAILED_PAGES"
            fi
          else
            echo "✅ All tests pass and all pages achieve 100% lighthouse scores"
          fi

          # Set outputs
          echo "overall_issues=$OVERALL_ISSUES" >> $GITHUB_OUTPUT
          echo "lighthouse_issues=$LIGHTHOUSE_ISSUES" >> $GITHUB_OUTPUT
          echo "test_failures=$([[ "$TEST_PASSED" != "true" ]] && echo true || echo false)" >> $GITHUB_OUTPUT
          echo "failed_pages<<EOF" >> $GITHUB_OUTPUT
          echo -e "$FAILED_PAGES" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        id: check_issues

      - name: "📧 Create issue for test/lighthouse failures"
        if: steps.check_issues.outputs.overall_issues == 'true'
        uses: actions/github-script@v7
        env:
          FAILED_PAGES: ${{ steps.check_issues.outputs.failed_pages }}
          TARGET_URL: ${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}
          BRANCH_NAME: ${{ github.ref_name }}
          LIGHTHOUSE_ISSUES: ${{ steps.check_issues.outputs.lighthouse_issues }}
          TEST_FAILURES: ${{ steps.check_issues.outputs.test_failures }}
          TEST_PASSED: ${{ env.TEST_PASSED }}
          TEST_SUCCESS_COUNT: ${{ env.TEST_SUCCESS_COUNT }}
          TEST_TOTAL_COUNT: ${{ env.TEST_TOTAL_COUNT }}
        with:
          script: |
            const failedPages = process.env.FAILED_PAGES;
            const targetUrl = process.env.TARGET_URL;
            const branchName = process.env.BRANCH_NAME;
            const hasLighthouseIssues = process.env.LIGHTHOUSE_ISSUES === 'true';
            const hasTestFailures = process.env.TEST_FAILURES === 'true';
            const testPassed = process.env.TEST_PASSED === 'true';
            const testSuccessCount = process.env.TEST_SUCCESS_COUNT || '0';
            const testTotalCount = process.env.TEST_TOTAL_COUNT || '0';

            // Determine branch-specific label and issue type
            const branchLabel = branchName === 'develop' ? 'dev branch' : 'production-build';
            const issueType = hasTestFailures && hasLighthouseIssues ? 'test-lighthouse' :
                            hasTestFailures ? 'test' : 'lighthouse';

            // Check for existing open issues for this specific branch and type
            const searchLabels = hasTestFailures ?
              `monitoring,${branchLabel},test` :
              `monitoring,${branchLabel},lighthouse`;

            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: searchLabels,
              per_page: 5
            });

            if (existingIssues.data.length > 0) {
              const existingIssue = existingIssues.data[0];
              console.log(`✅ Skipping duplicate issue - open ${issueType} issue exists for ${branchName}: #${existingIssue.number}`);

              // Update existing issue with latest report link
              const updateBody = `\n\n---\n**Latest Report Update:** ${new Date().toISOString()}\n📊 [View latest reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`;

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: updateBody
              });

              return;
            }

            // Build issue title and body based on failure type
            let issueTitle;
            let issueBody = [];

            if (hasTestFailures && hasLighthouseIssues) {
              issueTitle = `🚨 Test & Lighthouse Failures - ${branchName} - ${new Date().toISOString().split('T')[0]}`;
              issueBody = [
                "## 🚨 Test & Lighthouse Failure Alert",
                "",
                `**Date:** ${new Date().toISOString()}`,
                `**Target:** ${targetUrl}`,
                `**Branch:** ${branchName}`,
                "",
                "### 🧪 Test Results",
                `- **Status:** ❌ FAILED`,
                `- **Passed:** ${testSuccessCount}/${testTotalCount}`,
                `- **Failed:** ${testTotalCount - testSuccessCount}`,
                "",
                "### 🔦 Lighthouse Results",
                "🚨 Pages Not Achieving 100% Scores:",
                failedPages
              ];
            } else if (hasTestFailures) {
              issueTitle = `🧪 Test Failures - ${branchName} - ${new Date().toISOString().split('T')[0]}`;
              issueBody = [
                "## 🧪 Test Failure Alert",
                "",
                `**Date:** ${new Date().toISOString()}`,
                `**Target:** ${targetUrl}`,
                `**Branch:** ${branchName}`,
                "",
                "### 🚨 Test Results",
                `- **Status:** ❌ FAILED`,
                `- **Passed:** ${testSuccessCount}/${testTotalCount}`,
                `- **Failed:** ${testTotalCount - testSuccessCount}`
              ];
            } else {
              issueTitle = `🔦 Lighthouse Optimization Needed - ${branchName} - ${new Date().toISOString().split('T')[0]}`;
              issueBody = [
                "## 🔦 Lighthouse Score Optimization Alert",
                "",
                `**Date:** ${new Date().toISOString()}`,
                `**Target:** ${targetUrl}`,
                `**Branch:** ${branchName}`,
                "",
                "### 🚨 Pages Not Achieving 100% Scores:",
                failedPages
              ];
            }

            // Common sections
            issueBody.push(
              "",
              "### 📊 Full Report",
              `[View complete test & lighthouse reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`,
              "",
              "### 🔧 Next Steps"
            );

            if (hasTestFailures) {
              issueBody.push(
                "#### For Test Failures:",
                "1. Run tests locally: `npm run test`",
                "2. Review failed test output for specific error messages",
                "3. Check for recent code changes that might affect tests",
                "4. Verify all mocks and test setup are correct",
                ""
              );
            }

            if (hasLighthouseIssues) {
              issueBody.push(
                "#### For Lighthouse Issues:",
                "1. Review the detailed Lighthouse reports",
                "2. Identify specific optimization opportunities",
                "3. Fix accessibility, performance, SEO, or best practice issues",
                "4. Re-run monitoring to verify 100% scores",
                ""
              );
            }

            issueBody.push(
              "### 🎯 Goals",
              hasTestFailures ? "- ✅ All tests must pass" : "",
              hasLighthouseIssues ? "- 🔦 Achieve 100% scores across all Lighthouse categories:" : "",
              hasLighthouseIssues ? "  - Performance: 100%" : "",
              hasLighthouseIssues ? "  - Accessibility: 100%" : "",
              hasLighthouseIssues ? "  - Best Practices: 100%" : "",
              hasLighthouseIssues ? "  - SEO: 100%" : "",
              "",
              "---",
              "*This issue was automatically created by the enhanced Test + Lighthouse monitoring workflow*"
            );

            // Set appropriate labels
            const labels = ['monitoring', branchLabel];
            if (hasTestFailures) labels.push('test', 'bug');
            if (hasLighthouseIssues) labels.push('lighthouse', 'performance');

            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: issueTitle,
              body: issueBody.filter(line => line !== "").join('\n'),
              labels: labels
            });

  # Send notification summary
  notify:
    needs: lighthouse-audit
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/develop' || github.ref == 'refs/heads/main')

    steps:
      - name: "📊 Workflow Summary"
        run: |
          # For now, only targeting dev environment
          TARGET_URL="${{ github.event.inputs.target_url || 'https://dev.f4tdaddy.com' }}"

          echo "## 🔦 Lighthouse Monitoring Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ needs.lighthouse-audit.result }}" >> $GITHUB_STEP_SUMMARY
          echo "**Target:** $TARGET_URL" >> $GITHUB_STEP_SUMMARY
          echo "**Time:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📊 [View Reports](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
